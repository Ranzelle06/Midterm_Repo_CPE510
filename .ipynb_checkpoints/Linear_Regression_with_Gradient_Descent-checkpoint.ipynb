{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Linear Regression with Gradient Descent\n",
    "\n",
    "- Loading and looking at data\n",
    "- Cost function\n",
    "- Gradient Descent\n",
    "\n",
    "Packages:  \n",
    "`Pandas` for data frames and easy to read csv files  \n",
    "`Numpy` for array and matrix mathematics functions  \n",
    "`Matplotlib` for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('ex1data1.txt', names = ['population', 'profit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split population and profit into X and y\n",
    "X_df = pd.DataFrame(data.population)\n",
    "y_df = pd.DataFrame(data.profit)\n",
    "\n",
    "## Length, or number of observations, in our data\n",
    "m = len(y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(X_df, y_df, 'kx')\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.ylabel('Profit in $10,000s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of linear regression is to find a relationship between our target or dependent variable (y) and a set of explanatory variables ($x_1, x_2...$). This relatonship can then be used to predict other values.\n",
    "\n",
    "In our case with one variable, this relationship is a line defined by parameters $\\beta$ and the following form: $y = \\beta_0 + \\beta_1x$, where $\\beta_0$ is our intercept.\n",
    "\n",
    "This can be extended to multivariable regression by extending the equation in vector form: $y=X\\beta$\n",
    "\n",
    "So how do we make the best line? In this figure, there are many possible lines. Which one is the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(X_df, y_df, 'k.')\n",
    "plt.plot([5, 22], [6,6], '-')\n",
    "plt.plot([5, 22], [0,20], '-')\n",
    "plt.plot([5, 15], [-5,25], '-')\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.ylabel('Profit in $10,000s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "It turns out that to make the best line to model the data, we want to pick parameters $\\beta$ that allows our predicted value to be as close to the actual value as possible. In other words, we want the distance or residual between our hypothesis $h(x)$ and y to be minimized.\n",
    "\n",
    "So we formally define a cost function using ordinary least squares that is simply the sum of the squared distances. To find the liner regression line, we minimize:\n",
    "$$J(\\beta) = \\frac{1}{2m}\\sum_{i=1}^m(h_\\beta(x^{(i)})-y^{(i)})^2$$\n",
    "\n",
    "Again the hypothesis that we're trying to find is given by the linear model:\n",
    "$$h_\\beta(x) = \\beta^{T}x = \\beta_0 + \\beta_1x_1$$\n",
    "\n",
    "The parameters of the model are the beta values. We adjust $\\beta_j$ to minimze the cost function $J(\\beta)$. \n",
    "\n",
    "And we can use batch gradient descent where each iteration performs the update\n",
    "$$\\beta_j := \\beta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m (h_\\beta(x^{(i)})-y^{(i)})x_{j}^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa, what's gradient descent? And why are we updating that?\n",
    "\n",
    "Gradient descent simply is an algorithm that makes small steps along a function to find a local minimum. We can look at a simply quadratic equation such as this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_quad = [n/10 for n in range(0, 100)]\n",
    "y_quad = [(n-4)**2+5 for n in x_quad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,7))\n",
    "plt.plot(x_quad, y_quad, 'k--')\n",
    "plt.axis([0,10,0,30])\n",
    "plt.plot([1, 2, 3], [14, 9, 6], 'ro')\n",
    "plt.plot([5, 7, 8],[6, 14, 21], 'bo')\n",
    "plt.plot(4, 5, 'ko')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Quadratic Equation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're trying to find the local minimum on this function. If we start at the first red dot at x = 2, we find the gradient and we move against it. In this case, the gradient is the slope. And since the slope is negative, our next attempt is further to the right. Thus bringing us closer to the minimum. \n",
    "\n",
    "Indeed, we keep updating our parameter beta to get us closer and closer to the minumum.\n",
    "$$\\beta_j := \\beta_j - \\alpha\\frac{\\partial}{\\partial \\beta_j} J(\\beta)$$\n",
    "Where $\\alpha$ is our learning rate and $J(\\beta)$ is our cost function. By adjusting alpha, we can change how quickly we converge on the minimum (at the risk of overshooting it entirely and does not converge on our local minimum)\n",
    "\n",
    "Derive into the final formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's try to implement this in Python. First we declare some parameters. Alpha is the learning rate, and iterations defines how many times we want to perform the update.\n",
    "\n",
    "Then we transform the data frame holding our data into an array for simpler matrix math. And then write a helper function to calculate the cost function as defined above. Using np.dot for inner matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1500\n",
    "alpha = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a columns of 1s as intercept to X\n",
    "X_df['intercept'] = 1\n",
    "\n",
    "## Transform to Numpy arrays for easier matrix math and start theta at 0\n",
    "X = np.array(X_df)\n",
    "y = np.array(y_df).flatten()\n",
    "theta = np.array([0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X, y, theta):\n",
    "    \"\"\"\n",
    "    cost_function(X, y, theta) computes the cost of using theta as the\n",
    "    parameter for linear regression to fit the data points in X and y\n",
    "    \"\"\"\n",
    "    ## number of training examples\n",
    "    m = len(y) \n",
    "    \n",
    "    ## Calculate the cost with the given parameters\n",
    "    J = np.sum((X.dot(theta)-y)**2)/2/m\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we split the gradient descent algorithm into 4 parts so that we can see what's going on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    \"\"\"\n",
    "    gradient_descent Performs gradient descent to learn theta\n",
    "    theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by \n",
    "    taking num_iters gradient steps with learning rate alpha\n",
    "    \"\"\"\n",
    "    cost_history = [0] * iterations\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        hypothesis = X.dot(theta)\n",
    "        loss = hypothesis-y\n",
    "        gradient = X.T.dot(loss)/m\n",
    "        theta = theta - alpha*gradient\n",
    "        cost = cost_function(X, y, theta)\n",
    "        cost_history[iteration] = cost\n",
    "\n",
    "    return theta, cost_history\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(t, c) = gradient_descent(X,y,theta,alpha, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print theta parameters\n",
    "print (t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction\n",
    "print (np.array([3.5, 1]).dot(t))\n",
    "print (np.array([7, 1]).dot(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the best fit line\n",
    "best_fit_x = np.linspace(0, 25, 20)\n",
    "best_fit_y = [t[1] + t[0]*xx for xx in best_fit_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(X_df.population, y_df, '.')\n",
    "plt.plot(best_fit_x, best_fit_y, '-')\n",
    "plt.axis([0,25,-5,25])\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.ylabel('Profit in $10,000s')\n",
    "plt.title('Profit vs. Population with Linear Regression Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
